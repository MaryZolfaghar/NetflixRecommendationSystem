{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Predict ratings using Spectral Clustering\n",
    "https://towardsdatascience.com/unsupervised-machine-learning-spectral-clustering-algorithm-implemented-from-scratch-in-python-205c87271045\n",
    "\n",
    "https://medium.com/@amelie_yeh/singular-value-decomposition-low-rank-approximation-5d867bf67404"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.linalg import svd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from numpy import linalg as LA\n",
    "from scipy.sparse import linalg\n",
    "from scipy.linalg import eig as LAeig\n",
    "\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "from sklearn.metrics.pairwise import sigmoid_kernel, cosine_similarity\n",
    "from sklearn.cluster import SpectralClustering, KMeans, MiniBatchKMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arguments():\n",
    "    # Set Path\n",
    "    DATAPATH = '../../datasets/'\n",
    "    RESULTPATH = '../results/'\n",
    "    metadata = 'False'\n",
    "    fillnan='mean_col'\n",
    "    sim_method='cosine_similarity'\n",
    "    norm_laplacian_k=5\n",
    "    normalize_laplacian='False'\n",
    "    kmeans_k=5\n",
    "    n_epochs=10\n",
    "    test_prc=0.25\n",
    "    graph_nodes='M'\n",
    "    \n",
    "args = arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading and preprocessing data\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "def read_preprocss_data(args):\n",
    "    time_start=time.time()\n",
    "    # args.DATAPATH = '../datasets/'\n",
    "    train = pd.read_csv(args.DATAPATH + 'train.csv')\n",
    "    test = pd.read_csv(args.DATAPATH + 'test.csv')\n",
    "\n",
    "    train.columns = ['movie_id', 'customer_id', 'rating', 'date']\n",
    "    test.columns  = ['movie_id', 'customer_id', 'rating', 'date']\n",
    "\n",
    "    df = train.pivot_table(index='customer_id', \\\n",
    "                               columns='movie_id', values='rating', aggfunc=np.mean).fillna(0)\n",
    "    A_fill_zeros = df.to_numpy().copy()\n",
    "\n",
    "    if args.fillnan=='mean_col':\n",
    "        df = train.pivot_table(index='customer_id', \\\n",
    "                               columns='movie_id', values='rating', aggfunc=np.mean)\n",
    "        A = df.to_numpy().copy()\n",
    "        # column mean\n",
    "        col_mean = np.nanmean(A, axis = 0)\n",
    "        col_mean = np.ceil(col_mean)\n",
    "        print(col_mean.shape)\n",
    "        col_mean[col_mean>5]=5\n",
    "        # find indices where nan value is present\n",
    "        inds = np.where(np.isnan(A))\n",
    "        # replace inds with avg of column\n",
    "        A[inds] = np.take(col_mean, inds[1])\n",
    "    elif args.fillnan=='mean_row':\n",
    "        df = train.pivot_table(index='customer_id', \\\n",
    "                               columns='movie_id', values='rating', aggfunc=np.mean)\n",
    "        A = df.to_numpy().copy()\n",
    "        # row mean\n",
    "        row_mean = np.nanmean(A, axis = 1)\n",
    "        row_mean = np.ceil(row_mean)\n",
    "        # find indices where nan value is present\n",
    "        inds = np.where(np.isnan(A))\n",
    "        # replace inds with avg of column\n",
    "        A[inds] = np.take(row_mean, inds[1])\n",
    "\n",
    "    print('Reading time elapsed: {} sec'.format(time.time()-time_start))\n",
    "    print('Reading is done, the shape of the data is:', A.shape)\n",
    "    \n",
    "\n",
    "    return df, A, A_fill_zeros\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generating similarity matrix\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import sigmoid_kernel, cosine_similarity\n",
    "\n",
    "\n",
    "def gen_similarity(args, X):\n",
    "\n",
    "    if args.sim_method=='sigmoid_kernel':\n",
    "        sim_UXU=sigmoid_kernel(X=X, Y=None, gamma=None, coef0=1)\n",
    "        sim_MXM=sigmoid_kernel(X=X.T, Y=None, gamma=None, coef0=1)\n",
    "    elif args.sim_method=='cosine_similarity':\n",
    "        sim_UXU=cosine_similarity(X=X, Y=None)\n",
    "        sim_MXM=cosine_similarity(X=X.T, Y=None)\n",
    "    ## =====================================================================\n",
    "#     # Save similarity matrix\n",
    "#     fn_str = args.RESULTPATH + 'sim_%s_UXU.npy' %(args.sim_method)\n",
    "#     with open(fn_str, 'wb') as f:\n",
    "#         pickle.dump(sim_UXU, f)\n",
    "\n",
    "#     fn_str = args.RESULTPATH + 'sim_%s_MXM.npy' %(args.sim_method)\n",
    "#     with open(fn_str, 'wb') as f:\n",
    "#         pickle.dump(sim_MXM, f)\n",
    "#     print('saving similarity matrix is done!')\n",
    "    ## =====================================================================\n",
    "    return sim_UXU, sim_MXM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calculating the Laplacian matrix\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def calc_laplacian(args, Ws):\n",
    "    t1 = time.time()\n",
    "    # degree matrix\n",
    "    D = np.diag(np.sum(np.array(Ws), axis=1))\n",
    "    print('degree matrix:')\n",
    "    print(D.shape)\n",
    "    # laplacian matrix\n",
    "    L = D - Ws\n",
    "    print('laplacian matrix:')\n",
    "    print(L.shape)\n",
    "    elapsed_time = time.time() - t1\n",
    "    print('Elapsed time is {} seconds: '.format(elapsed_time))\n",
    "\n",
    "    return L, D\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calculate eigen vectors and values of the input\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# from numpy import linalg as LA\n",
    "# from scipy.sparse import linalg\n",
    "# from scipy.linalg import eig as LAeig\n",
    "# from scipy import linalg\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "\n",
    "def calc_eig(args, L, Ws, kk):\n",
    "    t1 = time.time()\n",
    "    D=np.diag(np.sum(Ws, axis=0))\n",
    "    vol=np.sum(np.diag(D))\n",
    "\n",
    "    vals, vecs = eigsh(L, k=kk, which=\"SM\")  # Largest 5 eigenvalues/vectors\n",
    "    vecs = vecs.real\n",
    "\n",
    "#     vals, vecs = np.linalg.eig(L)\n",
    "#     vecs = vecs.real\n",
    "#     vals = vals[np.argsort(vals)]\n",
    "#     vals = vals[1:]\n",
    "#     vecs = vecs[:,np.argsort(vals)]\n",
    "\n",
    "    print('the first 10 eigen values are:')\n",
    "    print(vals[:10])\n",
    "    print('\\n')\n",
    "\n",
    "    if (vals[0]==0):\n",
    "        if vals[1] > 0:\n",
    "            print('OOOPS the first eigen value was zero')\n",
    "            vals = vals[1:]\n",
    "            vecs = vecs[:,1:]\n",
    "    if (vals[0]<1e-10):\n",
    "        print('OOOPS the first eigen value was so small')\n",
    "        vals = vals[1:]\n",
    "        vecs = vecs[:,1:]\n",
    "\n",
    "    #caluclate eigen gap\n",
    "    e1 = np.zeros([vals.shape[0]+1])\n",
    "    e2 = np.zeros([vals.shape[0]+1])\n",
    "    print(e1.shape)\n",
    "    e1[1:] = vals.copy()\n",
    "    e2[:-1] = vals.copy()\n",
    "    print('eigen gap is:')\n",
    "    eigengap=(e2-e1)\n",
    "    print(eigengap)\n",
    "    print('the first 10 eigen values are:')\n",
    "    print(vals[:10])\n",
    "    print('\\n')\n",
    "    #\n",
    "\n",
    "\n",
    "    # eigenvalues\n",
    "    print('eigenvalues shape is:')\n",
    "    print(vals.shape)\n",
    "    # eigenvectors\n",
    "    print('eigenvectors shape is :')\n",
    "    print(vecs.shape)\n",
    "    if args.normalize_laplacian:\n",
    "        print('do the normalization')\n",
    "        Y = np.sort(vals)\n",
    "        I = np.argsort(vals)\n",
    "        v_norm = vecs[:,I[:args.norm_laplacian_k]] \\\n",
    "            / LA.norm(vecs[:,I[:args.norm_laplacian_k]])*vol**(1/2)\n",
    "    else:\n",
    "        v_norm = []\n",
    "    elapsed_time = time.time() - t1\n",
    "    print('Elapsed time is {} seconds: '.format(elapsed_time))\n",
    "    print('calc eigen vectors and values done!')\n",
    "    return vals, vecs, v_norm, eigengap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16142,)\n",
      "Reading time elapsed: 17.44917106628418 sec\n",
      "Reading is done, the shape of the data is: (5905, 16142)\n",
      "done reading the data\n",
      "data shape is: (5905, 16142)\n",
      "data fill zero shape is: (5905, 16142)\n"
     ]
    }
   ],
   "source": [
    "df, A, A_fill_zeros = read_preprocss_data(args)\n",
    "print('done reading the data')\n",
    "\n",
    "data = A.copy()\n",
    "data_fill_zeros = A_fill_zeros.copy()\n",
    "print('data shape is:', data.shape)\n",
    "print('data fill zero shape is:', data_fill_zeros.shape)\n",
    "#===========================================================================\n",
    "#=======================================================================\n",
    "test = pd.read_csv(args.DATAPATH + 'test.csv')\n",
    "test.columns  = ['movie_id', 'customer_id', 'rating', 'date']\n",
    "test_np = test.to_numpy().copy()\n",
    "\n",
    "train = pd.read_csv(args.DATAPATH + 'train.csv')\n",
    "train.columns  = ['movie_id', 'customer_id', 'rating', 'date']\n",
    "train_np = train.to_numpy().copy()\n",
    "\n",
    "train_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_to_index = {movie:indx for indx, movie in enumerate(df.columns.values)}\n",
    "index_to_movie = {indx:movie for indx, movie in enumerate(df.columns.values)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #===========================================================================\n",
    "# # use a subset of data just for testing everything first\n",
    "# nu=10 # number of users\n",
    "# ni=20 # number of items\n",
    "# A_temp = A.copy()\n",
    "# data = A_temp[:nu,:ni] # small 10 X 20 submatrix\n",
    "# print(data.shape)\n",
    "\n",
    "# A_temp = A_fill_zeros.copy()\n",
    "# data_fill_zeros = A_temp[:nu,:ni] # small 10 X 20 submatrix\n",
    "\n",
    "# train_np = train_np[:nu,:ni]\n",
    "# test_np = test_np[:nu,:ni]\n",
    "\n",
    "# train_data = data.copy()\n",
    "\n",
    "# test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen similarity is done\n",
      "degree matrix:\n",
      "(16142, 16142)\n",
      "laplacian matrix:\n",
      "(16142, 16142)\n",
      "Elapsed time is 12.831299066543579 seconds: \n",
      "calc laplacian is done\n"
     ]
    }
   ],
   "source": [
    "#===========================================================================\n",
    "# STEP 4 - Using the k smallest eigenvector as input,\n",
    "# train a k-means model and use it to classify the data\n",
    "#===========================================================================\n",
    "if args.graph_nodes=='M':\n",
    "    n_k = [10, 50, 100]\n",
    "elif args.graph_nodes=='U':\n",
    "    n_k = [10, 50, 100]\n",
    "#=======================================================================\n",
    "final_k = 5\n",
    "#=======================================================================\n",
    "# STEP 1 - Calculate similarity\n",
    "sim_UXU, sim_MXM = gen_similarity(args, train_data)\n",
    "print('gen similarity is done')\n",
    "\n",
    "# STEP 2 - computing the laplacian\n",
    "if args.graph_nodes=='M':\n",
    "    Ws = sim_MXM.copy()\n",
    "elif args.graph_nodes=='U':\n",
    "    Ws = sim_UXU.copy()\n",
    "L, D = calc_laplacian(args, Ws)\n",
    "print('calc laplacian is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first 10 eigen values are:\n",
      "[-1.03364969e-11  1.58044353e+04  1.58622461e+04  1.58665676e+04\n",
      "  1.58736045e+04]\n",
      "\n",
      "\n",
      "OOOPS the first eigen value was so small\n",
      "(5,)\n",
      "eigen gap is:\n",
      "[ 1.58044353e+04  5.78107852e+01  4.32158062e+00  7.03690276e+00\n",
      " -1.58736045e+04]\n",
      "the first 10 eigen values are:\n",
      "[15804.43527071 15862.24605587 15866.56763649 15873.60453925]\n",
      "\n",
      "\n",
      "eigenvalues shape is:\n",
      "(4,)\n",
      "eigenvectors shape is :\n",
      "(16142, 4)\n",
      "do the normalization\n",
      "Elapsed time is 12.0967698097229 seconds: \n",
      "calc eigen vectors and values done!\n",
      "U array eigenvectors shape: (16142, 4)\n",
      "MiniBatchKMeans time elapsed: 0.01572394371032715 sec\n",
      "MiniBatchKMeans Fit time elapsed: 0.23460006713867188 sec\n"
     ]
    }
   ],
   "source": [
    "# STEP 3 - Compute the eigenvectors of the matrix L\n",
    "vals, vecs, v_norm, eigengap = calc_eig(args, L, Ws, final_k)\n",
    "\n",
    "# STEP 5 - using k centers to predict data\n",
    "U = np.array(vecs)\n",
    "print('U array eigenvectors shape:', U.shape)\n",
    "\n",
    "t1=time.time()\n",
    "km = MiniBatchKMeans(n_clusters=final_k,\n",
    "                     random_state=0,\n",
    "                     batch_size=100,\n",
    "                     max_iter=100)\n",
    "print('MiniBatchKMeans time elapsed: {} sec'.format(time.time()-t1))\n",
    "km.fit(U)\n",
    "print('MiniBatchKMeans Fit time elapsed: {} sec'.format(time.time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249999, 4)\n",
      "ic: 0\n",
      "[[11279 5858 5.0 '2005-05-07']\n",
      " [12140 5202 '?' '2004-01-24']\n",
      " [12191 18818 '?' '2005-03-27']\n",
      " ...\n",
      " [16707 32275 '?' '2004-07-29']\n",
      " [17339 10728 '?' '2005-07-15']\n",
      " [17580 5600 '?' '2005-06-18']]\n",
      "Creating movie titles time elapsed: 0.48276686668395996 sec\n",
      "ic: 100000\n",
      "[[11279 5858 5.0 '2005-05-07']\n",
      " [12140 5202 4.0 '2004-01-24']\n",
      " [12191 18818 4.0 '2005-03-27']\n",
      " ...\n",
      " [16707 32275 '?' '2004-07-29']\n",
      " [17339 10728 '?' '2005-07-15']\n",
      " [17580 5600 '?' '2005-06-18']]\n",
      "Creating movie titles time elapsed: 537.4410078525543 sec\n",
      "ic: 200000\n",
      "[[11279 5858 5.0 '2005-05-07']\n",
      " [12140 5202 4.0 '2004-01-24']\n",
      " [12191 18818 4.0 '2005-03-27']\n",
      " ...\n",
      " [16707 32275 '?' '2004-07-29']\n",
      " [17339 10728 '?' '2005-07-15']\n",
      " [17580 5600 '?' '2005-06-18']]\n",
      "Creating movie titles time elapsed: 1018.030720949173 sec\n",
      "existed: 119\n"
     ]
    }
   ],
   "source": [
    "print(test_np.shape)\n",
    "\n",
    "if args.graph_nodes=='M': # menas the sim is MXM\n",
    "    labels = np.zeros([final_k])\n",
    "    pred_ratings = np.zeros(train_data.shape[1])\n",
    "    t0=time.time()\n",
    "    for il, lbl in enumerate(range(final_k)):\n",
    "        dfz=data_fill_zeros[:,km.labels_==lbl].copy()\n",
    "\n",
    "        # find user that rated at least one of the movies\n",
    "        goodU= np.mean(dfz, axis=1)\n",
    "        if goodU.shape[0] > 0:\n",
    "            # index for users that rate at least one of\n",
    "            # the movies in that clustr\n",
    "            indxgu=np.where(goodU > 0)\n",
    "            trdata = train_data[:, km.labels_==lbl]\n",
    "            trdata = trdata[indxgu[0], :]\n",
    "        else:\n",
    "            trdata = train_data[:, km.labels_==lbl]\n",
    "\n",
    "        trdata = np.mean(trdata,axis=0)\n",
    "        pr = np.ceil(np.mean(trdata,axis=0))\n",
    "         \n",
    "        if pr > 5:\n",
    "                labels[il] = 5\n",
    "        else:\n",
    "                labels[il] = pr\n",
    "    \n",
    "    existed=0\n",
    "    time_start=time.time()\n",
    "    labels2=labels.copy()\n",
    "    for ic in range(len(test_np)):    \n",
    "        mvid   = test_np[ic, 0]\n",
    "        custid = test_np[ic, 1]\n",
    "        if mvid not in movie_to_index.keys():\n",
    "            test_np[ic,2] = -1\n",
    "            continue\n",
    "        existed_rate = train[(train[\"movie_id\"]==mvid)&(train[\"customer_id\"]==custid)]\n",
    "        if (existed_rate.empty):\n",
    "            indx = movie_to_index[mvid]\n",
    "            ctst = km.labels_[indx]\n",
    "            test_np[ic,2] = labels[ctst]\n",
    "        else:\n",
    "            existed+=1\n",
    "            test_np[ic,2] = existed_rate\n",
    "        if ic%100000==0:\n",
    "            print('ic:', ic)\n",
    "            print(test_np)\n",
    "            # Save movie titles\n",
    "            fn_str = args.RESULTPATH + 'test_np_spectralClustring2'\n",
    "            with open(fn_str, 'wb') as f:\n",
    "                pickle.dump(test_np, f)\n",
    "            print('Creating movie titles time elapsed: {} sec'.format(time.time()-time_start))\n",
    "    print('existed:', existed)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating movie titles time elapsed: 2278.0017879009247 sec\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.DataFrame(data=test_np, columns=['movie_id', 'customer_id', 'rating', 'date'])\n",
    "fn_str = args.RESULTPATH + 'MaryZolfaghar_preds_clustering_k5.csv'\n",
    "with open(fn_str, 'wb') as f:\n",
    "    pickle.dump(test_df, f)\n",
    "print('Creating movie titles time elapsed: {} sec'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test_df = pd.DataFrame(data=test_np, columns=['movie_id', 'customer_id', 'rating', 'date'])\n",
    "# fn_str = args.RESULTPATH + 'test_np_spectralClustring_df_4k.csv'\n",
    "# with open(fn_str, 'rb') as f:\n",
    "#     test_ans = pickle.load( f)\n",
    "# test_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visulization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data2 = data.copy()\n",
    "\n",
    "if args.graph_nodes=='M': # menas the sim is MXM\n",
    "    labels = np.zeros([final_k])\n",
    "    pred_ratings = np.zeros(train_data.shape[1])\n",
    "    t0=time.time()\n",
    "    for il, lbl in enumerate(range(final_k)):\n",
    "        dfz=data_fill_zeros[:,km.labels_==lbl].copy()\n",
    "\n",
    "        # find user that rated at least one of the movies\n",
    "        goodU= np.mean(dfz, axis=1)\n",
    "        if goodU.shape[0] > 0:\n",
    "            # index for users that rate at least one of\n",
    "            # the movies in that clustr\n",
    "            indxgu=np.where(goodU > 0)\n",
    "            trdata = train_data[:, km.labels_==lbl]\n",
    "            trdata = trdata[indxgu[0], :]\n",
    "        else:\n",
    "            trdata = train_data[:, km.labels_==lbl]\n",
    "\n",
    "        trdata = np.mean(trdata,axis=0)\n",
    "#         labels[il] = np.ceil(np.mean(trdata,axis=0))\n",
    "#         \n",
    "        pr = np.ceil(np.mean(trdata,axis=0))\n",
    "         \n",
    "        if pr > 5:\n",
    "                labels[il] = 5\n",
    "            else:\n",
    "                labels[il] = pr\n",
    "    \n",
    "    existed=0\n",
    "    time_start=time.time()\n",
    "    labels2=labels.copy()\n",
    "    \n",
    "    time_start=time.time()\n",
    "    for ic in range(train_data.shape[1]):\n",
    "        ctst = km.labels_[ic]\n",
    "        labels2=labels.copy()\n",
    "        pred_ratings[ic] = labels2[ctst]\n",
    "        train_data2[:,ic] = labels2[ctst]\n",
    "\n",
    "        if ic%5000==0:\n",
    "            print('ic:', ic)\n",
    "            print(train_data2)\n",
    "            print('\\n')\n",
    "            print(pred_ratings)\n",
    "            print('\\n')\n",
    "            fn_str = args.RESULTPATH + 'train_data2_filled_spectralClustring'\n",
    "            with open(fn_str, 'wb') as f:\n",
    "                pickle.dump(train_data2, f)\n",
    "            print('Creating movie titles time elapsed: {} sec'.format(time.time()-time_start))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def plot_clusters(data_plot, labels, labels_txt, final_k):\n",
    "    time_start = time.time()\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(data_plot)\n",
    "\n",
    "    xx = tsne_results[:,0]\n",
    "    yy = tsne_results[:,1]\n",
    "\n",
    "    plt.figure(figsize=(14,10))\n",
    "    sns.scatterplot(\n",
    "        x=xx, y=yy,\n",
    "        hue=labels,\n",
    "        palette=sns.color_palette(\"hls\", final_k),\n",
    "        legend=\"full\",\n",
    "        alpha=0.3)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(z, y)\n",
    "\n",
    "    for i, txt in enumerate(n):\n",
    "        ax.annotate(txt, (z[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=[]; t2=[]; t3=[]\n",
    "with open(args.DATAPATH + 'movie_titles.txt', 'r',encoding=\"latin-1\") as reading:\n",
    "    for line in reading.readlines():\n",
    "        tokens = line.split(\",\")\n",
    "        t1.append(tokens[0])\n",
    "        t2.append(tokens[1])\n",
    "        t33 = tokens[2].split('\\n')\n",
    "        t3.append(t33[0])\n",
    "\n",
    "t1=np.asarray(t1)\n",
    "t1=t1[1:]\n",
    "t2=np.asarray(t2)\n",
    "t2=t2[1:]\n",
    "t3=np.asarray(t3)\n",
    "t3=t3[1:]\n",
    "\n",
    "titles = pd.read_fwf(args.DATAPATH + 'movie_titles.txt', delimiter= ',', \\\n",
    "                           names = [\"movie_id\", \"year_produced\", \"title\"], encoding=\"ISO-8859-1\")\n",
    "\n",
    "\n",
    "movie_titles = pd.DataFrame(titles[1:], columns=[\"movie_id\", \"year_produced\", \"title\"])\n",
    "\n",
    "movie_titles['movie_id'] = t1\n",
    "movie_titles['year_produced'] = t2\n",
    "movie_titles['title'] = t3\n",
    "\n",
    "movie_titles\n",
    "\n",
    "movieid_to_title = {movie:title for in enumerate(movie_titles)}\n",
    "movieid_to_title = {movie:year for in enumerate(movie_titles)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = U.copy()\n",
    "km_labels = km.labels_\n",
    "labels_txt=''\n",
    "print('data_plot shape:', labels.shape)\n",
    " \n",
    "plot_clusters(data_plot, labels, labels_txt, final_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearnigEEG",
   "language": "python",
   "name": "deeplearnigeeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
